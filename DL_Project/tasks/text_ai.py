# tasks/text_ai.py
import torch
import torch.nn as nn
from transformers import BertModel, BertTokenizer

# =========================
# í™˜ê²½ ì„¤ì • (ë…¸íŠ¸ë¶ ê¸°ì¤€)
# =========================
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"

PRETRAINED = "skt/kobert-base-v1"
MAX_LEN = 254                 # ë…¸íŠ¸ë¶ì—ì„œ ì‚¬ìš©í•œ ê°’
NUM_LABELS = 2                # 0=Human, 1=AI

WEIGHT_PATH = "weights/text_kobert.pt"   # best_head_only.pt ë³µì‚¬í•´ì„œ ì‚¬ìš©


# =========================
# 1) ëª¨ë¸ ì •ì˜ (ë…¸íŠ¸ë¶ ê·¸ëŒ€ë¡œ)
# =========================
class KoBERTLinearHeadClassifier(nn.Module):
    def __init__(self):
        super().__init__()

        self.bert = BertModel.from_pretrained(PRETRAINED)

        # backbone freeze
        for p in self.bert.parameters():
            p.requires_grad = False

        self.classifier = nn.Sequential(
            nn.Linear(768, 256),
            nn.GELU(),
            nn.Dropout(0.3),
            nn.Linear(256, NUM_LABELS)
        )

    def forward(self, input_ids, attention_mask):
        outputs = self.bert(
            input_ids=input_ids,
            attention_mask=attention_mask
        )
        cls = outputs.last_hidden_state[:, 0, :]  # [CLS]
        logits = self.classifier(cls)
        return logits


# =========================
# 2) í† í¬ë‚˜ì´ì € / ëª¨ë¸ ë¡œë”© (ğŸ”¥ import ì‹œ 1íšŒ)
# =========================
tokenizer = BertTokenizer.from_pretrained(PRETRAINED)

model = KoBERTLinearHeadClassifier().to(DEVICE)
model.load_state_dict(
    torch.load(WEIGHT_PATH, map_location=DEVICE)
)
model.eval()


# =========================
# 3) ì „ì²˜ë¦¬ (ë…¸íŠ¸ë¶ Dataset ë¡œì§)
# =========================
def preprocess(text: str):
    """
    raw text â†’ KoBERT input tensor
    """
    enc = tokenizer(
        text,
        truncation=True,
        padding="max_length",
        max_length=MAX_LEN,
        return_tensors="pt"
    )

    return {
        "input_ids": enc["input_ids"].to(DEVICE),
        "attention_mask": enc["attention_mask"].to(DEVICE)
    }


# =========================
# 4) GUIì—ì„œ í˜¸ì¶œí•˜ëŠ” í•¨ìˆ˜ (â­ í•µì‹¬)
# =========================
@torch.no_grad()
def predict(text: str):
    """
    Returns:
        {
          label: int,          # 0=Human, 1=AI
          confidence: float,
          detail: dict
        }
    """
    if text is None or not str(text).strip():
        return {
            "label": -1,
            "confidence": 0.0,
            "detail": {"error": "Empty input"}
        }

    inputs = preprocess(text)
    logits = model(**inputs)
    probs = torch.softmax(logits, dim=1).squeeze(0)

    pred_label = int(torch.argmax(probs))
    confidence = float(probs[pred_label])

    return {
        "label": pred_label,
        "confidence": confidence,
        "detail": {
            "human_prob": float(probs[0]),
            "ai_prob": float(probs[1])
        }
    }

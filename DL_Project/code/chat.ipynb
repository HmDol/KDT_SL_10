{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 259,
   "id": "dfc2bf22",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 1. 모듈 로딩\n",
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# 요청하신 모듈들\n",
    "from torchmetrics import classification as clf\n",
    "from torchmetrics.classification import BinaryF1Score, BinaryAccuracy\n",
    "from konlpy.tag import Okt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "id": "ab81e5b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.DataFrame'>\n",
      "RangeIndex: 7623 entries, 0 to 7622\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype\n",
      "---  ------  --------------  -----\n",
      " 0   text    7623 non-null   str  \n",
      " 1   label   7623 non-null   int64\n",
      "dtypes: int64(1), str(1)\n",
      "memory usage: 119.2 KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "## 2. 데이터 로드 및 정제 (제공해주신 코드 + 정제 로직 추가)\n",
    "lines = []\n",
    "files = [\n",
    "    \"./DATA/사람-사람 대화.txt\",\n",
    "    \"./DATA/사람-사람 대화2.txt\"  \n",
    "]\n",
    "\n",
    "# 사람 데이터 읽기\n",
    "for file in files:\n",
    "    try:\n",
    "        with open(file, encoding=\"utf-8\") as f:\n",
    "            for line in f:\n",
    "                line = line.strip()\n",
    "                if not line: continue\n",
    "                # 맨 앞 번호 및 특수문자 제거\n",
    "                line = re.sub(r\"^\\d+[\\.\\:\\)]?\\s*\", \"\", line)\n",
    "                if line:\n",
    "                    lines.append(line)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"파일을 찾을 수 없음: {file}\")\n",
    "\n",
    "human_df = pd.DataFrame({\"text\": lines, \"label\": 0})\n",
    "print(human_df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b088ddd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "파일 없음: ./DATA/output_daily_1st.json\n",
      "<class 'pandas.DataFrame'>\n",
      "RangeIndex: 13341 entries, 0 to 13340\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype\n",
      "---  ------  --------------  -----\n",
      " 0   text    13341 non-null  str  \n",
      " 1   label   13341 non-null  int64\n",
      "dtypes: int64(1), str(1)\n",
      "memory usage: 208.6 KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "json_files = [\n",
    "    \"./DATA/output_daily_1st.json\",\n",
    "    \"./DATA/output_daily_2nd.json\",\n",
    "]\n",
    "\n",
    "json_lines = []\n",
    "\n",
    "for jf in json_files:\n",
    "    try:\n",
    "        with open(jf, encoding=\"utf-8\") as f:\n",
    "            data = json.load(f)   # list of dict\n",
    "            \n",
    "            for item in data:\n",
    "                utter = item.get(\"user_utterance\")\n",
    "                \n",
    "                # null, None, 빈 문자열 제거\n",
    "                if utter and utter != \"null\":\n",
    "                    utter = utter.strip()\n",
    "                    if utter:\n",
    "                        json_lines.append(utter)\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"파일 없음: {jf}\")\n",
    "    \n",
    "json_df = pd.DataFrame({\n",
    "    \"text\": json_lines,\n",
    "    \"label\": 0   # 사람 데이터\n",
    "})\n",
    "\n",
    "# 기존 txt 기반 human_df와 병합\n",
    "human_df = pd.concat([human_df, json_df], ignore_index=True)\n",
    "\n",
    "print(human_df.info())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "id": "d8e44c80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.DataFrame'>\n",
      "RangeIndex: 12000 entries, 0 to 11999\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype\n",
      "---  ------  --------------  -----\n",
      " 0   text    12000 non-null  str  \n",
      " 1   label   12000 non-null  int64\n",
      "dtypes: int64(1), str(1)\n",
      "memory usage: 187.6 KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# AI 데이터 읽기\n",
    "try:\n",
    "    ai_df = pd.read_csv(\"./DATA/사람-AI 대화.csv\")\n",
    "    # 컬럼명이 '대화문'이 아니라면 수정 필요 (파일 확인 결과 'text'일 수 있음)\n",
    "    if '대화문' in ai_df.columns:\n",
    "        ai_df = ai_df.rename(columns={\"대화문\": \"text\"})\n",
    "    \n",
    "    # [중요] AI 답변에서 질문 태그(<s>[INST]...[/INST]) 제거 함수\n",
    "    def clean_ai_text(text):\n",
    "        if pd.isna(text): return \"\"\n",
    "        text = str(text)\n",
    "        if \"[/INST]\" in text:\n",
    "            text = text.split(\"[/INST]\")[-1] # 태그 뒷부분(답변)만 사용\n",
    "        text = text.replace(\"</s>\", \"\").replace(\"<s>\", \"\").strip()\n",
    "        text = re.sub(r\"^['\\\"]|['\\\"]$\", \"\", text) # 앞뒤 따옴표 제거\n",
    "        return text\n",
    "\n",
    "    ai_df['text'] = ai_df['text'].apply(clean_ai_text)\n",
    "    ai_df[\"label\"] = 1\n",
    "except Exception as e:\n",
    "    print(f\"AI 데이터 로드 중 오류: {e}\")\n",
    "    ai_df = pd.DataFrame(columns=['text', 'label'])\n",
    "\n",
    "print(ai_df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "id": "f806d3c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 데이터 개수: 25341\n"
     ]
    }
   ],
   "source": [
    "# 데이터 병합\n",
    "df = pd.concat([human_df, ai_df], ignore_index=True)\n",
    "df = df.dropna(subset=['text']) # 결측치 제거\n",
    "df = df[df['text'].str.strip() != \"\"] # 빈 문자열 제거\n",
    "print(f\"전체 데이터 개수: {len(df)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "id": "7fba340b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "0    13341\n",
       "1    12000\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 265,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"label\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "id": "9be44548",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "단어 사전 크기: 27024\n"
     ]
    }
   ],
   "source": [
    "okt = Okt()\n",
    "\n",
    "# 텍스트 -> 형태소 리스트 변환\n",
    "X_data = []\n",
    "for sentence in df['text']:\n",
    "    # 속도를 위해 stem=True (어간 추출) 사용\n",
    "    tokenized = okt.morphs(sentence, stem=True) \n",
    "    X_data.append(tokenized)\n",
    "\n",
    "# 단어 사전(Vocabulary) 만들기\n",
    "word_to_index = {\"<PAD>\": 0, \"<UNK>\": 1} # 0: 패딩, 1: 모르는 단어\n",
    "for sent in X_data:\n",
    "    for word in sent:\n",
    "        if word not in word_to_index:\n",
    "            word_to_index[word] = len(word_to_index)\n",
    "\n",
    "vocab_size = len(word_to_index)\n",
    "print(f\"단어 사전 크기: {vocab_size}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "id": "6a5817c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 4. 텍스트 인코딩 및 패딩\n",
    "def text_to_sequence(tokenized_sentences, vocab, max_len=50):\n",
    "    sequences = []\n",
    "    for sent in tokenized_sentences:\n",
    "        # 단어를 인덱스로 변환 (없으면 1: <UNK>)\n",
    "        seq = [vocab.get(word, 1) for word in sent]\n",
    "        \n",
    "        # 패딩 (길이 맞추기)\n",
    "        if len(seq) < max_len:\n",
    "            seq = seq + [0] * (max_len - len(seq)) # 뒤에 0 채우기\n",
    "        else:\n",
    "            seq = seq[:max_len] # 자르기\n",
    "        sequences.append(seq)\n",
    "    return torch.tensor(sequences, dtype=torch.long)\n",
    "\n",
    "MAX_LEN = 50  # 문장 최대 길이 설정\n",
    "X_tensor = text_to_sequence(X_data, word_to_index, MAX_LEN)\n",
    "y_tensor = torch.tensor(df['label'].values, dtype=torch.float32).unsqueeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "id": "c0c04749",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습/검증 데이터 분리\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_tensor, y_tensor, test_size=0.2, random_state=42, stratify=y_tensor\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "id": "14d60a9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataLoader 생성\n",
    "train_ds = TensorDataset(X_train, y_train)\n",
    "val_ds = TensorDataset(X_val, y_val)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=64, shuffle=True)\n",
    "val_loader = DataLoader(val_ds, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "id": "4bde23a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, dropout=0.3):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n",
    "\n",
    "        self.fc1 = nn.Linear(hidden_dim, 32)\n",
    "        self.fc2 = nn.Linear(32, 16)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        self.out = nn.Linear(16, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: [B, T]\n",
    "        x = self.embedding(x)              # [B, T, E]\n",
    "\n",
    "        _, (hidden, _) = self.lstm(x)\n",
    "        x = hidden[-1]                     # [B, H]\n",
    "\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "\n",
    "        return torch.sigmoid(self.out(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "id": "46aa83a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 하이퍼파라미터\n",
    "epochs = 30\n",
    "LR = 0.001\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "ALL_MODEL     = './Models/text_model.pt'   \n",
    "WEIGHTS_MODEL = './Models/text_weights' \n",
    "\n",
    "\n",
    "\n",
    "model = TextClassifier(vocab_size, 100, 64).to(device)\n",
    "loss_fn = nn.BCELoss() # 이진 분류 손실함수\n",
    "optimizer = optim.Adam(model.parameters(), lr=LR)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "id": "2c986a21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/30] Train Loss: 0.2508 | Val Loss: 0.0980 | Val Acc: 0.9629 | Val F1: 0.9595\n",
      "  --> Best Model Saved! (Loss: 0.0980)\n",
      "Epoch [2/30] Train Loss: 0.0718 | Val Loss: 0.0778 | Val Acc: 0.9751 | Val F1: 0.9735\n",
      "  --> Best Model Saved! (Loss: 0.0778)\n",
      "Epoch [3/30] Train Loss: 0.0489 | Val Loss: 0.0637 | Val Acc: 0.9799 | Val F1: 0.9786\n",
      "  --> Best Model Saved! (Loss: 0.0637)\n",
      "Epoch [4/30] Train Loss: 0.0305 | Val Loss: 0.0856 | Val Acc: 0.9803 | Val F1: 0.9791\n",
      "Epoch [5/30] Train Loss: 0.0253 | Val Loss: 0.0816 | Val Acc: 0.9781 | Val F1: 0.9766\n",
      "Epoch [6/30] Train Loss: 0.0192 | Val Loss: 0.0796 | Val Acc: 0.9815 | Val F1: 0.9803\n",
      "Epoch [7/30] Train Loss: 0.0161 | Val Loss: 0.0803 | Val Acc: 0.9805 | Val F1: 0.9793\n",
      "Epoch [8/30] Train Loss: 0.0255 | Val Loss: 0.0880 | Val Acc: 0.9803 | Val F1: 0.9791\n",
      "Epoch [9/30] Train Loss: 0.0291 | Val Loss: 0.0986 | Val Acc: 0.9749 | Val F1: 0.9738\n",
      "Epoch [10/30] Train Loss: 0.0332 | Val Loss: 0.1094 | Val Acc: 0.9730 | Val F1: 0.9714\n",
      "Epoch [11/30] Train Loss: 0.0413 | Val Loss: 0.0808 | Val Acc: 0.9817 | Val F1: 0.9805\n",
      "Epoch [12/30] Train Loss: 0.0215 | Val Loss: 0.0740 | Val Acc: 0.9809 | Val F1: 0.9796\n",
      "Epoch [13/30] Train Loss: 0.0133 | Val Loss: 0.0712 | Val Acc: 0.9820 | Val F1: 0.9809\n",
      "Epoch [14/30] Train Loss: 0.0116 | Val Loss: 0.0781 | Val Acc: 0.9826 | Val F1: 0.9816\n",
      "Epoch [15/30] Train Loss: 0.0083 | Val Loss: 0.0981 | Val Acc: 0.9801 | Val F1: 0.9790\n",
      "Epoch [16/30] Train Loss: 0.0071 | Val Loss: 0.1010 | Val Acc: 0.9805 | Val F1: 0.9793\n",
      "Epoch [17/30] Train Loss: 0.0072 | Val Loss: 0.0871 | Val Acc: 0.9813 | Val F1: 0.9800\n",
      "Epoch [18/30] Train Loss: 0.0064 | Val Loss: 0.1187 | Val Acc: 0.9797 | Val F1: 0.9785\n",
      "Epoch [19/30] Train Loss: 0.0163 | Val Loss: 0.0793 | Val Acc: 0.9787 | Val F1: 0.9773\n",
      "Epoch [20/30] Train Loss: 0.0104 | Val Loss: 0.0983 | Val Acc: 0.9787 | Val F1: 0.9775\n",
      "Epoch [21/30] Train Loss: 0.0069 | Val Loss: 0.0996 | Val Acc: 0.9801 | Val F1: 0.9790\n",
      "Epoch [22/30] Train Loss: 0.0061 | Val Loss: 0.1142 | Val Acc: 0.9783 | Val F1: 0.9772\n",
      "Epoch [23/30] Train Loss: 0.0074 | Val Loss: 0.1076 | Val Acc: 0.9819 | Val F1: 0.9808\n",
      "Epoch [24/30] Train Loss: 0.0064 | Val Loss: 0.0819 | Val Acc: 0.9817 | Val F1: 0.9806\n",
      "Epoch [25/30] Train Loss: 0.0060 | Val Loss: 0.0931 | Val Acc: 0.9819 | Val F1: 0.9808\n",
      "Epoch [26/30] Train Loss: 0.0057 | Val Loss: 0.1070 | Val Acc: 0.9815 | Val F1: 0.9804\n",
      "Epoch [27/30] Train Loss: 0.0053 | Val Loss: 0.1030 | Val Acc: 0.9807 | Val F1: 0.9794\n",
      "Epoch [28/30] Train Loss: 0.0056 | Val Loss: 0.1003 | Val Acc: 0.9795 | Val F1: 0.9781\n",
      "Epoch [29/30] Train Loss: 0.0058 | Val Loss: 0.0909 | Val Acc: 0.9813 | Val F1: 0.9801\n",
      "Epoch [30/30] Train Loss: 0.0107 | Val Loss: 0.1074 | Val Acc: 0.9799 | Val F1: 0.9786\n",
      "학습 완료!\n"
     ]
    }
   ],
   "source": [
    "# TorchMetrics 평가 지표 정의\n",
    "metric_acc = BinaryAccuracy().to(device)\n",
    "metric_f1 = BinaryF1Score().to(device)\n",
    "\n",
    "best_val_loss = float('inf')\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    \n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = loss_fn(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item() \n",
    "# 검증 (Validation)\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = loss_fn(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "            \n",
    "            # 평가 지표 업데이트\n",
    "            metric_acc.update(outputs, labels)\n",
    "            metric_f1.update(outputs, labels)\n",
    "    \n",
    "    # 에포크 결과 계산\n",
    "    avg_train_loss = train_loss / len(train_loader)\n",
    "    avg_val_loss = val_loss / len(val_loader)\n",
    "    val_acc = metric_acc.compute()\n",
    "    val_f1 = metric_f1.compute()\n",
    "    \n",
    "    print(f\"Epoch [{epoch+1}/{epochs}] \"\n",
    "          f\"Train Loss: {avg_train_loss:.4f} | Val Loss: {avg_val_loss:.4f} | \"\n",
    "          f\"Val Acc: {val_acc:.4f} | Val F1: {val_f1:.4f}\")\n",
    "   \n",
    "    # Validation Loss가 개선되었을 때만 모델 저장 (Best Model)\n",
    "    if avg_val_loss < best_val_loss:\n",
    "        best_val_loss = avg_val_loss\n",
    "        torch.save(model.state_dict(), WEIGHTS_MODEL)\n",
    "        print(f\"  --> Best Model Saved! (Loss: {best_val_loss:.4f})\")\n",
    "    \n",
    "    # 지표 초기화\n",
    "    metric_acc.reset()\n",
    "    metric_f1.reset()\n",
    "\n",
    "print(\"학습 완료!\")\n",
    "# 전체 모델 저장 (마지막 상태)\n",
    "torch.save(model, ALL_MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfd15147",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "입력: '전에 자료 남아있어서 다시 안해도 되겠다'\n",
      "결과: 사람 (확률 0.9970)\n",
      "------------------------------\n",
      "입력: 'K디지털 트레이닝KDT은 고용노동부 주관의 국비 지원 디지털 직무 교육이야AI웹 개발데이터 분석 같은 분야에서 실무 중심으로 배울 수 있어'\n",
      "결과: AI (확률 0.9981)\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\keyin\\AppData\\Local\\Temp\\ipykernel_3368\\1984018874.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(WEIGHTS_MODEL))\n"
     ]
    }
   ],
   "source": [
    "## 7. 예측 테스트 함수\n",
    "model.load_state_dict(torch.load(WEIGHTS_MODEL))\n",
    "def predict_sentence(text):\n",
    "    model.eval()\n",
    "    # 전처리\n",
    "    text = re.sub(r\"[^가-힣a-zA-Z0-9\\s]\", \"\", text) # 특수문자 제거 등\n",
    "    tokenized = okt.morphs(text, stem=True)\n",
    "    seq = [word_to_index.get(w, 1) for w in tokenized]\n",
    "    \n",
    "    # 패딩\n",
    "    if len(seq) < MAX_LEN:\n",
    "        seq = seq + [0] * (MAX_LEN - len(seq))\n",
    "    else:\n",
    "        seq = seq[:MAX_LEN]\n",
    "        \n",
    "    input_tensor = torch.tensor([seq], dtype=torch.long).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        score = model(input_tensor).item()\n",
    "    \n",
    "    print(f\"입력: '{text}'\")\n",
    "    if score > 0.5:\n",
    "        print(f\"결과: AI (확률 {score:.4f})\")\n",
    "    else:\n",
    "        print(f\"결과: 사람 (확률 {1-score:.4f})\")\n",
    "    print(\"-\" * 30)\n",
    "\n",
    "# 테스트\n",
    "predict_sentence(\"전에 자료 남아있어서 다시 안해도 되겠다\")\n",
    "predict_sentence(\"K-디지털 트레이닝(KDT)**은 고용노동부 주관의 국비 지원 디지털 직무 교육이야.AI·웹 개발·데이터 분석 같은 분야에서 실무 중심으로 배울 수 있어.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "027f2c12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# [AI vs Human(자기소개서) 분류 프로젝트]\n",
    "# - KoBERT(backbone) freeze + Linear head 학습\n",
    "# - Train/Valid로 best 저장 후, best로 Test 1회 평가\n",
    "# =========================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6776b1e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\kdt008\\anaconda3\\envs\\NLP\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fa5669b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split complete\n",
      "[TRAIN] 128 {1: 65, 0: 63}\n",
      "[VALID] 16 {1: 8, 0: 8}\n",
      "[TEST ] 17 {1: 9, 0: 8}\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# [0] Train / Valid / Test Split \n",
    "# =========================\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "\n",
    "ALL_PATH   = \"./Data/all.csv\"           ## 전체 데이터셋 경로\n",
    "TRAIN_PATH = \"./Data/train.csv\"         ## Train 데이터 저장 경로\n",
    "VALID_PATH = \"./Data/valid.csv\"         ## valid 데이터 저장 경로\n",
    "TEST_PATH  = \"./Data/test.csv\"          ## test 데이터 저장 경로\n",
    "\n",
    "RANDOM_STATE = 42   ## 랜덤 상수 지정               \n",
    "VALID_SIZE = 0.1    ## train_valid_test = 8:1:1\n",
    "TEST_SIZE  = 0.1\n",
    "\n",
    "\n",
    "## 데이터셋 가져오기\n",
    "df = pd.read_csv(ALL_PATH, encoding=\"utf-8-sig\")\n",
    "\n",
    "\n",
    "## text, label 컬럼만 가져오기\n",
    "need_cols = {\"text\", \"label\"}\n",
    "if not need_cols.issubset(df.columns):\n",
    "    raise ValueError(f\"all.csv에는 최소 {need_cols} 컬럼이 필요함. 현재: {df.columns}\")\n",
    "\n",
    "# 1차 split: train vs temp\n",
    "train_df, temp_df = train_test_split(\n",
    "    df,\n",
    "    test_size=(VALID_SIZE + TEST_SIZE),\n",
    "    random_state=RANDOM_STATE,\n",
    "    stratify=df[\"label\"]\n",
    ")\n",
    "\n",
    "# 2차 split: valid vs test\n",
    "test_ratio = TEST_SIZE / (VALID_SIZE + TEST_SIZE)\n",
    "valid_df, test_df = train_test_split(\n",
    "    temp_df,\n",
    "    test_size=test_ratio,\n",
    "    random_state=RANDOM_STATE,\n",
    "    stratify=temp_df[\"label\"]\n",
    ")\n",
    "\n",
    "## 분리 후 데이터셋 저장\n",
    "train_df.to_csv(TRAIN_PATH, index=False, encoding=\"utf-8-sig\")\n",
    "valid_df.to_csv(VALID_PATH, index=False, encoding=\"utf-8-sig\")\n",
    "test_df.to_csv(TEST_PATH,  index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "\n",
    "print(\"Split complete\")\n",
    "print(\"[TRAIN]\", len(train_df), train_df[\"label\"].value_counts().to_dict())\n",
    "print(\"[VALID]\", len(valid_df), valid_df[\"label\"].value_counts().to_dict())\n",
    "print(\"[TEST ]\", len(test_df),  test_df[\"label\"].value_counts().to_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1b676303",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# 1) Utils: Seed 고정 (재현성)\n",
    "# =========================\n",
    "## 재현성을 위한 시드 고정\n",
    "def seed_everything(seed: int = 42) :\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1a3782c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================\n",
    "# 2) Dataset: CSV -> 토크나이즈 -> 텐서 반환\n",
    "# =============================================================\n",
    "class ResumeAIDataset(Dataset):\n",
    "    \"\"\"\n",
    "    KoBERT 분류용 Dataset\n",
    "    - 입력 CSV: text, label 컬럼 필요\n",
    "    - 출력: input_ids, attention_mask, labels\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, csv_path, tokenizer, max_len = 256):\n",
    "        \n",
    "        \n",
    "        ## 클래스 맴버 df 초기화\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        self.df = pd.read_csv(csv_path, encoding=\"utf-8-sig\")\n",
    "        \n",
    "        # 필수 컬럼 체크 -> 없을 시 value에러 일으킴\n",
    "        if \"text\" not in self.df.columns or \"label\" not in self.df.columns:\n",
    "            raise ValueError(f\"{csv_path}에는 'text', 'label' 컬럼이 필요합니다.\")\n",
    "\n",
    "        # 텍스트 최소 정리\n",
    "        self.df[\"text\"] = (\n",
    "            self.df[\"text\"]\n",
    "            .astype(str)\n",
    "            .str.replace(\"\\ufeff\", \"\") ## 엑셀 -> CSV 변환시 생기는 마크(BOM) 제거\n",
    "            .str.strip() ## 앞뒤 공백 제거\n",
    "        )\n",
    "\n",
    "        # 빈 텍스트 제거\n",
    "        self.df = self.df[self.df[\"text\"] != \"\"].reset_index(drop=True)\n",
    "\n",
    "        # label 정수화\n",
    "        self.df[\"label\"] = self.df[\"label\"].astype(int)\n",
    "\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx: int) -> dict:\n",
    "        text = self.df.loc[idx, \"text\"]\n",
    "        label = int(self.df.loc[idx, \"label\"])\n",
    "\n",
    "        enc = self.tokenizer(\n",
    "            text,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            max_length=self.max_len,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": enc[\"input_ids\"].squeeze(0),            # (L,)\n",
    "            \"attention_mask\": enc[\"attention_mask\"].squeeze(0),  # (L,)\n",
    "            \"labels\": torch.tensor(label, dtype=torch.long)      # ()\n",
    "        }\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a7251171",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# 3) Model: KoBERT freeze + Linear head\n",
    "# =========================\n",
    "class KoBERTLinearHeadClassifier(nn.Module):\n",
    "    \"\"\"\n",
    "    KoBERT : 한국어 문장을 \"의미 좌표(768차원 벡터)\"로 바꿔주는 거대한 뇌 언어\n",
    "    BERT : 문장의 의미를 벡터로 압축하는 모델\n",
    "    --> 이미 학습된 모델\n",
    "    \n",
    "    freeze의 의미 : KoBERT는 이미 학습되어 있으니 더이상 학습 X ==> 더이상 가중치 업데이터 X\n",
    "    --> 학습 속도 증가, 과적합 저하, 적은 데이터에 적합\n",
    "    \n",
    "    KoBERT는 고정(freeze)하고 head(Linear)만 학습하는 분류기\n",
    "    - forward 출력: logits (B, 2)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, pretrained_name = \"skt/kobert-base-v1\", num_labels = 2, dropout = 0.2):\n",
    "        super().__init__()\n",
    "\n",
    "        self.bert = BertModel.from_pretrained(pretrained_name)\n",
    "\n",
    "        # backbone freeze : 학습 안하겠다. \n",
    "        for p in self.bert.parameters():\n",
    "            p.requires_grad = False ## 가중치 업데이트 끄기\n",
    "        \n",
    "        # 마지막 encoder layer만 unfreeze\n",
    "        for name, p in self.bert.named_parameters():\n",
    "            if \"encoder.layer.11\" in name:\n",
    "                p.requires_grad = True\n",
    "\n",
    "        hidden = self.bert.config.hidden_size   # 768\n",
    "        self.dropout = nn.Dropout(dropout)      # 랜덤으로 20%를 0으로 만듦 -> 특정 신호의 의존 방어, train에만 적용, eval에는 off\n",
    "        # self.classifier = nn.Linear(hidden, num_labels)   ## 선형 모델로\n",
    "        self.classifier = nn.Sequential(    ## 비선형 모델로\n",
    "            nn.Linear(768, 512),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(0.3),\n",
    "\n",
    "            nn.Linear(512, 256),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(0.3),\n",
    "\n",
    "            nn.Linear(256, 128),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(0.3),\n",
    "\n",
    "            nn.Linear(128, 2)\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, input_ids: torch.Tensor, attention_mask: torch.Tensor) -> torch.Tensor:\n",
    "        out = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        cls = out.last_hidden_state[:, 0, :]  # (B, 768) : [CLS]\n",
    "        x = self.dropout(cls)\n",
    "        logits = self.classifier(x)           # (B, 2)\n",
    "        return logits\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "adf6c5eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# 4) Train / Evaluate\n",
    "# =========================\n",
    "def train_one_epoch(model: nn.Module,\n",
    "                    loader: DataLoader,\n",
    "                    optimizer: torch.optim.Optimizer,\n",
    "                    criterion: nn.Module,\n",
    "                    device: str) -> float:\n",
    "    \"\"\"1 epoch 학습\"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "\n",
    "    for batch in loader:\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(input_ids, attention_mask)\n",
    "        loss = criterion(logits, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    return total_loss / max(1, len(loader))\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model: nn.Module,\n",
    "             loader: DataLoader,\n",
    "             device: str):\n",
    "    \"\"\"\n",
    "    검증/테스트 평가\n",
    "    - accuracy\n",
    "    - macro F1 (데이터 적을 때/클래스 불균형에 비교적 안정)\n",
    "    - confusion matrix\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    y_true, y_pred = [], []\n",
    "\n",
    "    for batch in loader:\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        labels = batch[\"labels\"].cpu().numpy()\n",
    "\n",
    "        logits = model(input_ids, attention_mask)\n",
    "        preds = torch.argmax(logits, dim=1).cpu().numpy()\n",
    "\n",
    "        y_true.extend(labels.tolist())\n",
    "        y_pred.extend(preds.tolist())\n",
    "\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    f1_macro = f1_score(y_true, y_pred, average=\"macro\")\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    return acc, f1_macro, cm\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a35156fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEVICE: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading weights: 100%|██████████| 199/199 [00:00<00:00, 932.21it/s, Materializing param=pooler.dense.weight]                               \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 01] loss=0.7139 | val_acc=0.5000 | val_f1_macro=0.3333\n",
      "Confusion Matrix (VALID):\n",
      " [[0 8]\n",
      " [0 8]]\n",
      "[Epoch 02] loss=0.7060 | val_acc=0.5000 | val_f1_macro=0.3333\n",
      "Confusion Matrix (VALID):\n",
      " [[0 8]\n",
      " [0 8]]\n",
      "[Epoch 03] loss=0.7009 | val_acc=0.5000 | val_f1_macro=0.3333\n",
      "Confusion Matrix (VALID):\n",
      " [[8 0]\n",
      " [8 0]]\n",
      "[Epoch 04] loss=0.7085 | val_acc=0.5000 | val_f1_macro=0.3333\n",
      "Confusion Matrix (VALID):\n",
      " [[0 8]\n",
      " [0 8]]\n",
      "[Epoch 05] loss=0.7018 | val_acc=0.5000 | val_f1_macro=0.3333\n",
      "Confusion Matrix (VALID):\n",
      " [[0 8]\n",
      " [0 8]]\n",
      "[Epoch 06] loss=0.7087 | val_acc=0.5000 | val_f1_macro=0.3333\n",
      "Confusion Matrix (VALID):\n",
      " [[8 0]\n",
      " [8 0]]\n",
      "[Epoch 07] loss=0.6952 | val_acc=0.5000 | val_f1_macro=0.3333\n",
      "Confusion Matrix (VALID):\n",
      " [[0 8]\n",
      " [0 8]]\n",
      "[Epoch 08] loss=0.6955 | val_acc=0.5000 | val_f1_macro=0.3333\n",
      "Confusion Matrix (VALID):\n",
      " [[0 8]\n",
      " [0 8]]\n",
      "[Epoch 09] loss=0.6977 | val_acc=0.5000 | val_f1_macro=0.3333\n",
      "Confusion Matrix (VALID):\n",
      " [[0 8]\n",
      " [0 8]]\n",
      "[Epoch 10] loss=0.7019 | val_acc=0.5000 | val_f1_macro=0.3333\n",
      "Confusion Matrix (VALID):\n",
      " [[8 0]\n",
      " [8 0]]\n",
      "[Epoch 11] loss=0.6977 | val_acc=0.5000 | val_f1_macro=0.3333\n",
      "Confusion Matrix (VALID):\n",
      " [[0 8]\n",
      " [0 8]]\n",
      "[Epoch 12] loss=0.6925 | val_acc=0.5000 | val_f1_macro=0.3333\n",
      "Confusion Matrix (VALID):\n",
      " [[0 8]\n",
      " [0 8]]\n",
      "[Epoch 13] loss=0.7038 | val_acc=0.5000 | val_f1_macro=0.3333\n",
      "Confusion Matrix (VALID):\n",
      " [[0 8]\n",
      " [0 8]]\n",
      "[Epoch 14] loss=0.6964 | val_acc=0.5000 | val_f1_macro=0.3333\n",
      "Confusion Matrix (VALID):\n",
      " [[0 8]\n",
      " [0 8]]\n",
      "[Epoch 15] loss=0.7039 | val_acc=0.5000 | val_f1_macro=0.3333\n",
      "Confusion Matrix (VALID):\n",
      " [[0 8]\n",
      " [0 8]]\n",
      "[Epoch 16] loss=0.7005 | val_acc=0.5000 | val_f1_macro=0.3333\n",
      "Confusion Matrix (VALID):\n",
      " [[8 0]\n",
      " [8 0]]\n",
      "[Epoch 17] loss=0.6945 | val_acc=0.5000 | val_f1_macro=0.3333\n",
      "Confusion Matrix (VALID):\n",
      " [[0 8]\n",
      " [0 8]]\n",
      "[Epoch 18] loss=0.6971 | val_acc=0.5000 | val_f1_macro=0.3333\n",
      "Confusion Matrix (VALID):\n",
      " [[8 0]\n",
      " [8 0]]\n",
      "[Epoch 19] loss=0.6975 | val_acc=0.5000 | val_f1_macro=0.3333\n",
      "Confusion Matrix (VALID):\n",
      " [[0 8]\n",
      " [0 8]]\n",
      "[Epoch 20] loss=0.6939 | val_acc=0.5000 | val_f1_macro=0.3333\n",
      "Confusion Matrix (VALID):\n",
      " [[0 8]\n",
      " [0 8]]\n",
      "\n",
      "✅ Best VALID F1_macro: 0.3333333333333333\n",
      "✅ Saved: ./best_head_only.pt\n",
      "\n",
      "===== Load BEST model and Evaluate on TEST =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kdt008\\AppData\\Local\\Temp\\ipykernel_14616\\2367873747.py:75: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(SAVE_PATH, map_location=DEVICE))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TEST] acc=0.5294 | f1_macro=0.3462\n",
      "Confusion Matrix (TEST):\n",
      " [[0 8]\n",
      " [0 9]]\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# 5) Main: 설정 -> 데이터로더 -> 학습 -> best 저장 -> test 평가\n",
    "# =========================\n",
    "def main():\n",
    "    # --- 재현성 ---\n",
    "    seed_everything(42)\n",
    "\n",
    "    # --- 환경/모델 설정 ---\n",
    "    DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    PRETRAINED = \"skt/kobert-base-v1\"\n",
    "\n",
    "    # --- 데이터 경로 ---\n",
    "    TRAIN_PATH = \"./Data/train.csv\"\n",
    "    VALID_PATH = \"./Data/valid.csv\"\n",
    "    TEST_PATH  = \"./Data/test.csv\"\n",
    "\n",
    "    # --- 하이퍼파라미터 ---\n",
    "    MAX_LEN = 512\n",
    "    BATCH_SIZE = 4\n",
    "    EPOCHS = 20\n",
    "    LR = 1e-3\n",
    "    SAVE_PATH = \"./best_head_only.pt\"\n",
    "\n",
    "    print(\"DEVICE:\", DEVICE)\n",
    "\n",
    "    # --- 토크나이저/데이터셋/로더 ---\n",
    "    tokenizer = BertTokenizer.from_pretrained(PRETRAINED)\n",
    "\n",
    "    train_ds = ResumeAIDataset(TRAIN_PATH, tokenizer, max_len=MAX_LEN)\n",
    "    valid_ds = ResumeAIDataset(VALID_PATH, tokenizer, max_len=MAX_LEN)\n",
    "    test_ds  = ResumeAIDataset(TEST_PATH,  tokenizer, max_len=MAX_LEN)\n",
    "\n",
    "    train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    valid_loader = DataLoader(valid_ds, batch_size=BATCH_SIZE, shuffle=False)\n",
    "    test_loader  = DataLoader(test_ds,  batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "    # --- 모델/손실/옵티마이저 ---\n",
    "    model = KoBERTLinearHeadClassifier(pretrained_name=PRETRAINED).to(DEVICE)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # linear + kobert 마지막 leyer 튜닝\n",
    "    optimizer = torch.optim.AdamW(\n",
    "        [\n",
    "            {\"params\": model.classifier.parameters(), \"lr\": 1e-3},\n",
    "            {\n",
    "                \"params\": [\n",
    "                    p for n, p in model.bert.named_parameters()\n",
    "                    if \"encoder.layer.11\" in n\n",
    "                ],\n",
    "                \"lr\": 1e-5\n",
    "            }\n",
    "        ]\n",
    "    )\n",
    "\n",
    "\n",
    "    # --- 학습 루프 (best는 valid macro F1 기준 저장) ---\n",
    "    best_f1_macro = -1.0\n",
    "\n",
    "    for epoch in range(1, EPOCHS + 1):\n",
    "        train_loss = train_one_epoch(model, train_loader, optimizer, criterion, DEVICE)\n",
    "        val_acc, val_f1_macro, val_cm = evaluate(model, valid_loader, DEVICE)\n",
    "\n",
    "        print(f\"[Epoch {epoch:02d}] loss={train_loss:.4f} | val_acc={val_acc:.4f} | val_f1_macro={val_f1_macro:.4f}\")\n",
    "        print(\"Confusion Matrix (VALID):\\n\", val_cm)\n",
    "\n",
    "        if val_f1_macro > best_f1_macro:\n",
    "            best_f1_macro = val_f1_macro\n",
    "            torch.save(model.state_dict(), SAVE_PATH)\n",
    "\n",
    "    print(\"\\n✅ Best VALID F1_macro:\", best_f1_macro)\n",
    "    print(\"✅ Saved:\", SAVE_PATH)\n",
    "\n",
    "    # --- 테스트 평가는 best 모델로 1회 ---\n",
    "    print(\"\\n===== Load BEST model and Evaluate on TEST =====\")\n",
    "    model.load_state_dict(torch.load(SAVE_PATH, map_location=DEVICE))\n",
    "\n",
    "    test_acc, test_f1_macro, test_cm = evaluate(model, test_loader, DEVICE)\n",
    "    print(f\"[TEST] acc={test_acc:.4f} | f1_macro={test_f1_macro:.4f}\")\n",
    "    print(\"Confusion Matrix (TEST):\\n\", test_cm)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "027f2c12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# [AI vs Human(자기소개서) 분류 프로젝트]\n",
    "# - KoBERT(backbone) freeze + Linear head 학습\n",
    "# - Train/Valid로 best 저장 후, best로 Test 1회 평가\n",
    "# =========================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6776b1e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\kdt008\\anaconda3\\envs\\NLP\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# 0) Imports (최상단 통합)\n",
    "# =========================\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1b676303",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# 1) Utils: Seed 고정 (재현성)\n",
    "# =========================\n",
    "def seed_everything(seed: int = 42) -> None:\n",
    "    \"\"\"재현성을 위한 random seed 고정\"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1a3782c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# 2) Dataset: CSV -> 토크나이즈 -> 텐서 반환\n",
    "# =========================\n",
    "class ResumeAIDataset(Dataset):\n",
    "    \"\"\"\n",
    "    KoBERT 분류용 Dataset\n",
    "    - 입력 CSV: text, label 컬럼 필요\n",
    "    - 출력: input_ids, attention_mask, labels\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, csv_path: str, tokenizer: BertTokenizer, max_len: int = 256):\n",
    "        self.df = pd.read_csv(csv_path, encoding=\"utf-8-sig\")\n",
    "\n",
    "        # 필수 컬럼 체크\n",
    "        if \"text\" not in self.df.columns or \"label\" not in self.df.columns:\n",
    "            raise ValueError(f\"{csv_path}에는 'text', 'label' 컬럼이 필요합니다.\")\n",
    "\n",
    "        # 텍스트 최소 정리\n",
    "        self.df[\"text\"] = (\n",
    "            self.df[\"text\"]\n",
    "            .astype(str)\n",
    "            .str.replace(\"\\ufeff\", \"\")\n",
    "            .str.strip()\n",
    "        )\n",
    "\n",
    "        # 빈 텍스트 제거\n",
    "        self.df = self.df[self.df[\"text\"] != \"\"].reset_index(drop=True)\n",
    "\n",
    "        # label 정수화\n",
    "        self.df[\"label\"] = self.df[\"label\"].astype(int)\n",
    "\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx: int) -> dict:\n",
    "        text = self.df.loc[idx, \"text\"]\n",
    "        label = int(self.df.loc[idx, \"label\"])\n",
    "\n",
    "        enc = self.tokenizer(\n",
    "            text,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            max_length=self.max_len,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": enc[\"input_ids\"].squeeze(0),            # (L,)\n",
    "            \"attention_mask\": enc[\"attention_mask\"].squeeze(0),  # (L,)\n",
    "            \"labels\": torch.tensor(label, dtype=torch.long)      # ()\n",
    "        }\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a7251171",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# 3) Model: KoBERT freeze + Linear head\n",
    "# =========================\n",
    "class KoBERTLinearHeadClassifier(nn.Module):\n",
    "    \"\"\"\n",
    "    KoBERT는 고정(freeze)하고 head(Linear)만 학습하는 분류기\n",
    "    - forward 출력: logits (B, 2)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, pretrained_name: str = \"skt/kobert-base-v1\",\n",
    "                 num_labels: int = 2, dropout: float = 0.2):\n",
    "        super().__init__()\n",
    "\n",
    "        self.bert = BertModel.from_pretrained(pretrained_name)\n",
    "\n",
    "        # backbone freeze\n",
    "        for p in self.bert.parameters():\n",
    "            p.requires_grad = False\n",
    "\n",
    "        hidden = self.bert.config.hidden_size  # 768\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.classifier = nn.Linear(hidden, num_labels)\n",
    "\n",
    "    def forward(self, input_ids: torch.Tensor, attention_mask: torch.Tensor) -> torch.Tensor:\n",
    "        out = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        cls = out.last_hidden_state[:, 0, :]  # (B, 768) : [CLS]\n",
    "        x = self.dropout(cls)\n",
    "        logits = self.classifier(x)           # (B, 2)\n",
    "        return logits\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "adf6c5eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# 4) Train / Evaluate\n",
    "# =========================\n",
    "def train_one_epoch(model: nn.Module,\n",
    "                    loader: DataLoader,\n",
    "                    optimizer: torch.optim.Optimizer,\n",
    "                    criterion: nn.Module,\n",
    "                    device: str) -> float:\n",
    "    \"\"\"1 epoch 학습\"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "\n",
    "    for batch in loader:\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(input_ids, attention_mask)\n",
    "        loss = criterion(logits, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    return total_loss / max(1, len(loader))\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model: nn.Module,\n",
    "             loader: DataLoader,\n",
    "             device: str):\n",
    "    \"\"\"\n",
    "    검증/테스트 평가\n",
    "    - accuracy\n",
    "    - macro F1 (데이터 적을 때/클래스 불균형에 비교적 안정)\n",
    "    - confusion matrix\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    y_true, y_pred = [], []\n",
    "\n",
    "    for batch in loader:\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        labels = batch[\"labels\"].cpu().numpy()\n",
    "\n",
    "        logits = model(input_ids, attention_mask)\n",
    "        preds = torch.argmax(logits, dim=1).cpu().numpy()\n",
    "\n",
    "        y_true.extend(labels.tolist())\n",
    "        y_pred.extend(preds.tolist())\n",
    "\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    f1_macro = f1_score(y_true, y_pred, average=\"macro\")\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    return acc, f1_macro, cm\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a35156fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEVICE: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading weights: 100%|██████████| 199/199 [00:00<00:00, 1144.90it/s, Materializing param=pooler.dense.weight]                               \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 01] loss=0.7678 | val_acc=0.5000 | val_f1_macro=0.3333\n",
      "Confusion Matrix (VALID):\n",
      " [[0 3]\n",
      " [0 3]]\n",
      "[Epoch 02] loss=0.6894 | val_acc=0.5000 | val_f1_macro=0.3333\n",
      "Confusion Matrix (VALID):\n",
      " [[3 0]\n",
      " [3 0]]\n",
      "[Epoch 03] loss=0.6900 | val_acc=0.5000 | val_f1_macro=0.3333\n",
      "Confusion Matrix (VALID):\n",
      " [[3 0]\n",
      " [3 0]]\n",
      "[Epoch 04] loss=0.7228 | val_acc=0.5000 | val_f1_macro=0.3333\n",
      "Confusion Matrix (VALID):\n",
      " [[3 0]\n",
      " [3 0]]\n",
      "[Epoch 05] loss=0.7016 | val_acc=0.5000 | val_f1_macro=0.3333\n",
      "Confusion Matrix (VALID):\n",
      " [[0 3]\n",
      " [0 3]]\n",
      "[Epoch 06] loss=0.7089 | val_acc=0.5000 | val_f1_macro=0.3333\n",
      "Confusion Matrix (VALID):\n",
      " [[0 3]\n",
      " [0 3]]\n",
      "[Epoch 07] loss=0.6788 | val_acc=0.5000 | val_f1_macro=0.3333\n",
      "Confusion Matrix (VALID):\n",
      " [[0 3]\n",
      " [0 3]]\n",
      "[Epoch 08] loss=0.6821 | val_acc=0.5000 | val_f1_macro=0.3333\n",
      "Confusion Matrix (VALID):\n",
      " [[0 3]\n",
      " [0 3]]\n",
      "[Epoch 09] loss=0.6931 | val_acc=0.5000 | val_f1_macro=0.3333\n",
      "Confusion Matrix (VALID):\n",
      " [[0 3]\n",
      " [0 3]]\n",
      "[Epoch 10] loss=0.7018 | val_acc=0.6667 | val_f1_macro=0.6250\n",
      "Confusion Matrix (VALID):\n",
      " [[1 2]\n",
      " [0 3]]\n",
      "[Epoch 11] loss=0.7201 | val_acc=0.6667 | val_f1_macro=0.6250\n",
      "Confusion Matrix (VALID):\n",
      " [[1 2]\n",
      " [0 3]]\n",
      "[Epoch 12] loss=0.7125 | val_acc=0.5000 | val_f1_macro=0.3333\n",
      "Confusion Matrix (VALID):\n",
      " [[0 3]\n",
      " [0 3]]\n",
      "[Epoch 13] loss=0.7058 | val_acc=0.5000 | val_f1_macro=0.3333\n",
      "Confusion Matrix (VALID):\n",
      " [[0 3]\n",
      " [0 3]]\n",
      "[Epoch 14] loss=0.6882 | val_acc=0.5000 | val_f1_macro=0.3333\n",
      "Confusion Matrix (VALID):\n",
      " [[3 0]\n",
      " [3 0]]\n",
      "[Epoch 15] loss=0.6767 | val_acc=0.5000 | val_f1_macro=0.3333\n",
      "Confusion Matrix (VALID):\n",
      " [[3 0]\n",
      " [3 0]]\n",
      "[Epoch 16] loss=0.6792 | val_acc=0.6667 | val_f1_macro=0.6250\n",
      "Confusion Matrix (VALID):\n",
      " [[1 2]\n",
      " [0 3]]\n",
      "[Epoch 17] loss=0.7130 | val_acc=0.5000 | val_f1_macro=0.3333\n",
      "Confusion Matrix (VALID):\n",
      " [[0 3]\n",
      " [0 3]]\n",
      "[Epoch 18] loss=0.6958 | val_acc=0.5000 | val_f1_macro=0.3333\n",
      "Confusion Matrix (VALID):\n",
      " [[0 3]\n",
      " [0 3]]\n",
      "[Epoch 19] loss=0.7114 | val_acc=0.5000 | val_f1_macro=0.3333\n",
      "Confusion Matrix (VALID):\n",
      " [[0 3]\n",
      " [0 3]]\n",
      "[Epoch 20] loss=0.7053 | val_acc=0.5000 | val_f1_macro=0.3333\n",
      "Confusion Matrix (VALID):\n",
      " [[0 3]\n",
      " [0 3]]\n",
      "\n",
      "✅ Best VALID F1_macro: 0.625\n",
      "✅ Saved: ./best_head_only.pt\n",
      "\n",
      "===== Load BEST model and Evaluate on TEST =====\n",
      "[TEST] acc=0.5000 | f1_macro=0.3333\n",
      "Confusion Matrix (TEST):\n",
      " [[0 3]\n",
      " [0 3]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kdt008\\AppData\\Local\\Temp\\ipykernel_2688\\2926030768.py:63: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(SAVE_PATH, map_location=DEVICE))\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# 5) Main: 설정 -> 데이터로더 -> 학습 -> best 저장 -> test 평가\n",
    "# =========================\n",
    "def main():\n",
    "    # --- 재현성 ---\n",
    "    seed_everything(42)\n",
    "\n",
    "    # --- 환경/모델 설정 ---\n",
    "    DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    PRETRAINED = \"skt/kobert-base-v1\"\n",
    "\n",
    "    # --- 데이터 경로 ---\n",
    "    TRAIN_PATH = \"./Data/train.csv\"\n",
    "    VALID_PATH = \"./Data/valid.csv\"\n",
    "    TEST_PATH  = \"./Data/test.csv\"\n",
    "\n",
    "    # --- 하이퍼파라미터 ---\n",
    "    MAX_LEN = 256\n",
    "    BATCH_SIZE = 8\n",
    "    EPOCHS = 20\n",
    "    LR = 1e-3\n",
    "    SAVE_PATH = \"./best_head_only.pt\"\n",
    "\n",
    "    print(\"DEVICE:\", DEVICE)\n",
    "\n",
    "    # --- 토크나이저/데이터셋/로더 ---\n",
    "    tokenizer = BertTokenizer.from_pretrained(PRETRAINED)\n",
    "\n",
    "    train_ds = ResumeAIDataset(TRAIN_PATH, tokenizer, max_len=MAX_LEN)\n",
    "    valid_ds = ResumeAIDataset(VALID_PATH, tokenizer, max_len=MAX_LEN)\n",
    "    test_ds  = ResumeAIDataset(TEST_PATH,  tokenizer, max_len=MAX_LEN)\n",
    "\n",
    "    train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    valid_loader = DataLoader(valid_ds, batch_size=BATCH_SIZE, shuffle=False)\n",
    "    test_loader  = DataLoader(test_ds,  batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "    # --- 모델/손실/옵티마이저 ---\n",
    "    model = KoBERTLinearHeadClassifier(pretrained_name=PRETRAINED).to(DEVICE)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # ✅ head만 학습\n",
    "    optimizer = torch.optim.AdamW(model.classifier.parameters(), lr=LR)\n",
    "\n",
    "    # --- 학습 루프 (best는 valid macro F1 기준 저장) ---\n",
    "    best_f1_macro = -1.0\n",
    "\n",
    "    for epoch in range(1, EPOCHS + 1):\n",
    "        train_loss = train_one_epoch(model, train_loader, optimizer, criterion, DEVICE)\n",
    "        val_acc, val_f1_macro, val_cm = evaluate(model, valid_loader, DEVICE)\n",
    "\n",
    "        print(f\"[Epoch {epoch:02d}] loss={train_loss:.4f} | val_acc={val_acc:.4f} | val_f1_macro={val_f1_macro:.4f}\")\n",
    "        print(\"Confusion Matrix (VALID):\\n\", val_cm)\n",
    "\n",
    "        if val_f1_macro > best_f1_macro:\n",
    "            best_f1_macro = val_f1_macro\n",
    "            torch.save(model.state_dict(), SAVE_PATH)\n",
    "\n",
    "    print(\"\\n✅ Best VALID F1_macro:\", best_f1_macro)\n",
    "    print(\"✅ Saved:\", SAVE_PATH)\n",
    "\n",
    "    # --- 테스트 평가는 best 모델로 1회 ---\n",
    "    print(\"\\n===== Load BEST model and Evaluate on TEST =====\")\n",
    "    model.load_state_dict(torch.load(SAVE_PATH, map_location=DEVICE))\n",
    "\n",
    "    test_acc, test_f1_macro, test_cm = evaluate(model, test_loader, DEVICE)\n",
    "    print(f\"[TEST] acc={test_acc:.4f} | f1_macro={test_f1_macro:.4f}\")\n",
    "    print(\"Confusion Matrix (TEST):\\n\", test_cm)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "027f2c12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# [AI vs Human(자기소개서) 분류 프로젝트]\n",
    "# - KoBERT(backbone) freeze + Linear head 학습\n",
    "# - Train/Valid로 best 저장 후, best로 Test 1회 평가\n",
    "# =========================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6776b1e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\kdt008\\anaconda3\\envs\\NLP\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix\n",
    "from sklearn.model_selection import StratifiedKFold\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fa5669b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TRAINVAL] 144\n",
      "[TEST] 17\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# [0] Train / Valid / Test Split \n",
    "# =========================\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "\n",
    "ALL_PATH   = \"./Data/all.csv\"           ## 전체 데이터셋 경로\n",
    "TEST_PATH  = \"./Data/test.csv\"          ## test 데이터 저장 경로\n",
    "\n",
    "RANDOM_STATE = 42   ## 랜덤 상수 지정               \n",
    "TEST_SIZE  = 0.1\n",
    "\n",
    "\n",
    "df = pd.read_csv(ALL_PATH, encoding=\"utf-8-sig\")\n",
    "\n",
    "trainval_df, test_df = train_test_split(\n",
    "    df,\n",
    "    test_size=0.1,\n",
    "    random_state=42,\n",
    "    stratify=df[\"label\"]\n",
    ")\n",
    "\n",
    "test_df.to_csv(\"./Data/test.csv\", index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "print(\"[TRAINVAL]\", len(trainval_df))\n",
    "print(\"[TEST]\", len(test_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1b676303",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# 1) Utils: Seed 고정 (재현성)\n",
    "# =========================\n",
    "## 재현성을 위한 시드 고정\n",
    "def seed_everything(seed: int = 42) :\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1a3782c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================\n",
    "# 2) Dataset: CSV -> 토크나이즈 -> 텐서 반환\n",
    "# =============================================================\n",
    "class ResumeAIDataset(Dataset):\n",
    "    \"\"\"\n",
    "    KoBERT 분류용 Dataset\n",
    "    - 입력 CSV: text, label 컬럼 필요\n",
    "    - 출력: input_ids, attention_mask, labels\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, csv_path, tokenizer, max_len = 256):\n",
    "        \n",
    "        \n",
    "        ## 클래스 맴버 df 초기화\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        self.df = pd.read_csv(csv_path, encoding=\"utf-8-sig\")\n",
    "        \n",
    "        # 필수 컬럼 체크 -> 없을 시 value에러 일으킴\n",
    "        if \"text\" not in self.df.columns or \"label\" not in self.df.columns:\n",
    "            raise ValueError(f\"{csv_path}에는 'text', 'label' 컬럼이 필요합니다.\")\n",
    "\n",
    "        # 텍스트 최소 정리\n",
    "        self.df[\"text\"] = (\n",
    "            self.df[\"text\"]\n",
    "            .astype(str)\n",
    "            .str.replace(\"\\ufeff\", \"\") ## 엑셀 -> CSV 변환시 생기는 마크(BOM) 제거\n",
    "            .str.strip() ## 앞뒤 공백 제거\n",
    "        )\n",
    "\n",
    "        # 빈 텍스트 제거\n",
    "        self.df = self.df[self.df[\"text\"] != \"\"].reset_index(drop=True)\n",
    "\n",
    "        # label 정수화\n",
    "        self.df[\"label\"] = self.df[\"label\"].astype(int)\n",
    "\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx: int) -> dict:\n",
    "        text = self.df.loc[idx, \"text\"]\n",
    "        label = int(self.df.loc[idx, \"label\"])\n",
    "\n",
    "        enc = self.tokenizer(\n",
    "            text,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            max_length=self.max_len,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": enc[\"input_ids\"].squeeze(0),            # (L,)\n",
    "            \"attention_mask\": enc[\"attention_mask\"].squeeze(0),  # (L,)\n",
    "            \"labels\": torch.tensor(label, dtype=torch.long)      # ()\n",
    "        }\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a7251171",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# 3) Model: KoBERT freeze + Linear head\n",
    "# =========================\n",
    "class KoBERTLinearHeadClassifier(nn.Module):\n",
    "    \"\"\"\n",
    "    KoBERT : 한국어 문장을 \"의미 좌표(768차원 벡터)\"로 바꿔주는 거대한 뇌 언어\n",
    "    BERT : 문장의 의미를 벡터로 압축하는 모델\n",
    "    --> 이미 학습된 모델\n",
    "    \n",
    "    freeze의 의미 : KoBERT는 이미 학습되어 있으니 더이상 학습 X ==> 더이상 가중치 업데이터 X\n",
    "    --> 학습 속도 증가, 과적합 저하, 적은 데이터에 적합\n",
    "    \n",
    "    KoBERT는 고정(freeze)하고 head(Linear)만 학습하는 분류기\n",
    "    - forward 출력: logits (B, 2)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, pretrained_name = \"skt/kobert-base-v1\", num_labels = 2, dropout = 0.2):\n",
    "        super().__init__()\n",
    "\n",
    "        self.bert = BertModel.from_pretrained(pretrained_name)\n",
    "\n",
    "        # backbone freeze : 학습 안하겠다. \n",
    "        for p in self.bert.parameters():\n",
    "            p.requires_grad = False ## 가중치 업데이트 끄기\n",
    "\n",
    "        hidden = self.bert.config.hidden_size   # 768\n",
    "        self.dropout = nn.Dropout(dropout)      # 랜덤으로 20%를 0으로 만듦 -> 특정 신호의 의존 방어, train에만 적용, eval에는 off\n",
    "        # self.classifier = nn.Linear(hidden, num_labels)   ## 선형 모델로\n",
    "        self.classifier = nn.Sequential(    ## 비선형 모델로\n",
    "            nn.Linear(768, 512),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(0.3),\n",
    "\n",
    "            nn.Linear(512, 256),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(0.3),\n",
    "\n",
    "            nn.Linear(256, 128),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(0.3),\n",
    "\n",
    "            nn.Linear(128, 2)\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, input_ids: torch.Tensor, attention_mask: torch.Tensor) -> torch.Tensor:\n",
    "        out = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        cls = out.last_hidden_state[:, 0, :]  # (B, 768) : [CLS]\n",
    "        x = self.dropout(cls)\n",
    "        logits = self.classifier(x)           # (B, 2)\n",
    "        return logits\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "adf6c5eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# 4) Train / Evaluate\n",
    "# =========================\n",
    "def train_one_epoch(model: nn.Module,\n",
    "                    loader: DataLoader,\n",
    "                    optimizer: torch.optim.Optimizer,\n",
    "                    criterion: nn.Module,\n",
    "                    device: str) -> float:\n",
    "    \"\"\"1 epoch 학습\"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "\n",
    "    for batch in loader:\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(input_ids, attention_mask)\n",
    "        loss = criterion(logits, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    return total_loss / max(1, len(loader))\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model: nn.Module,\n",
    "             loader: DataLoader,\n",
    "             device: str):\n",
    "    \"\"\"\n",
    "    검증/테스트 평가\n",
    "    - accuracy\n",
    "    - macro F1 (데이터 적을 때/클래스 불균형에 비교적 안정)\n",
    "    - confusion matrix\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    y_true, y_pred = [], []\n",
    "\n",
    "    for batch in loader:\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        labels = batch[\"labels\"].cpu().numpy()\n",
    "\n",
    "        logits = model(input_ids, attention_mask)\n",
    "        preds = torch.argmax(logits, dim=1).cpu().numpy()\n",
    "\n",
    "        y_true.extend(labels.tolist())\n",
    "        y_pred.extend(preds.tolist())\n",
    "\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    f1_macro = f1_score(y_true, y_pred, average=\"macro\")\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    return acc, f1_macro, cm\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a35156fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Fold 1 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading weights: 100%|██████████| 199/199 [00:00<00:00, 981.27it/s, Materializing param=pooler.dense.weight]                               \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Fold 1] F1_macro = 0.3256\n",
      "\n",
      "===== Fold 2 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading weights: 100%|██████████| 199/199 [00:00<00:00, 1002.40it/s, Materializing param=pooler.dense.weight]                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Fold 2] F1_macro = 0.3256\n",
      "\n",
      "===== Fold 3 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading weights: 100%|██████████| 199/199 [00:00<00:00, 1009.14it/s, Materializing param=pooler.dense.weight]                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Fold 3] F1_macro = 0.3256\n",
      "\n",
      "===== Fold 4 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading weights: 100%|██████████| 199/199 [00:00<00:00, 1021.11it/s, Materializing param=pooler.dense.weight]                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Fold 4] F1_macro = 0.3256\n",
      "\n",
      "===== Fold 5 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading weights: 100%|██████████| 199/199 [00:00<00:00, 922.23it/s, Materializing param=pooler.dense.weight]                               \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Fold 5] F1_macro = 0.3333\n",
      "\n",
      "===== K-Fold Result =====\n",
      "F1 per fold: [0.32558139534883723, 0.32558139534883723, 0.32558139534883723, 0.32558139534883723, 0.3333333333333333]\n",
      "Mean F1 : 0.3271317829457364\n",
      "Std  F1 : 0.0031007751937984327\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def main():\n",
    "    seed_everything(42)\n",
    "\n",
    "    DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    PRETRAINED = \"skt/kobert-base-v1\"\n",
    "\n",
    "    MAX_LEN = 512\n",
    "    BATCH_SIZE = 4\n",
    "    EPOCHS = 15\n",
    "    LR = 1e-3\n",
    "\n",
    "    tokenizer = BertTokenizer.from_pretrained(PRETRAINED)\n",
    "\n",
    "    # --- K-Fold 설정 ---\n",
    "    skf = StratifiedKFold(\n",
    "        n_splits=5,\n",
    "        shuffle=True,\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "    df = trainval_df.reset_index(drop=True)\n",
    "    X = df[\"text\"].values\n",
    "    y = df[\"label\"].values\n",
    "\n",
    "\n",
    "    fold_f1_scores = []\n",
    "\n",
    "    for fold, (train_idx, val_idx) in enumerate(skf.split(X, y), 1):\n",
    "        print(f\"\\n===== Fold {fold} =====\")\n",
    "\n",
    "        train_df = df.iloc[train_idx].reset_index(drop=True)\n",
    "        val_df   = df.iloc[val_idx].reset_index(drop=True)\n",
    "\n",
    "        train_df.to_csv(\"train_fold.csv\", index=False, encoding=\"utf-8-sig\")\n",
    "        val_df.to_csv(\"val_fold.csv\", index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "        train_ds = ResumeAIDataset(\"train_fold.csv\", tokenizer, MAX_LEN)\n",
    "        val_ds   = ResumeAIDataset(\"val_fold.csv\", tokenizer, MAX_LEN)\n",
    "\n",
    "        train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True)\n",
    "        val_loader   = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "        # fold마다 새 모델\n",
    "        model = KoBERTLinearHeadClassifier(pretrained_name=PRETRAINED).to(DEVICE)\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = torch.optim.AdamW(model.classifier.parameters(), lr=LR)\n",
    "\n",
    "        # --- 학습 ---\n",
    "        for epoch in range(1, EPOCHS + 1):\n",
    "            train_loss = train_one_epoch(\n",
    "                model, train_loader, optimizer, criterion, DEVICE\n",
    "            )\n",
    "\n",
    "        # --- 평가 ---\n",
    "        _, val_f1, _ = evaluate(model, val_loader, DEVICE)\n",
    "        print(f\"[Fold {fold}] F1_macro = {val_f1:.4f}\")\n",
    "\n",
    "        fold_f1_scores.append(val_f1)\n",
    "\n",
    "    print(\"\\n===== K-Fold Result =====\")\n",
    "    print(\"F1 per fold:\", fold_f1_scores)\n",
    "    print(\"Mean F1 :\", np.mean(fold_f1_scores))\n",
    "    print(\"Std  F1 :\", np.std(fold_f1_scores))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

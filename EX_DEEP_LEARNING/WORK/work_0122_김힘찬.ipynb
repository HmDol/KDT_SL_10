{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "09cbe037",
   "metadata": {},
   "source": [
    "#### [26_01_22_과제]\n",
    "- 알파벳을 사용하는 언어는 알파벳 빈도의 차이로 언어를 식별할 수 있습니다.\n",
    "- 해당 데이터셋을 활용해서 언어 식별 모델을 생성하세요.\n",
    "- 데이터셋\n",
    "  * train 폴더 =>  나라영문2글자-숫자.txt\n",
    "  * test 폴더  =>  나라영문2글자-숫자.txt\n",
    "\n",
    "- 데이터셋 부족 시 Wikipedia 사이트에서 추가 가능 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "32692f53",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from collections import Counter\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f9e1a96a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 프랑스어 악센트 매핑\n",
    "accent_map = {\n",
    "    'é':'e','è':'e','ê':'e','ë':'e',\n",
    "    'à':'a','â':'a',\n",
    "    'ç':'c',\n",
    "    'î':'i','ï':'i',\n",
    "    'ô':'o',\n",
    "    'ù':'u','û':'u',\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "52a08a98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_files: ['en-1.txt', 'en-2.txt', 'en-3.txt', 'en-4.txt', 'en-5.txt'] ...\n",
      "test_files : ['en-1.txt', 'en-2.txt', 'fr-3.txt', 'fr-4.txt', 'id-5.txt'] ...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ============================================================\n",
    "# 1) train/test 폴더에서 파일 리스트 가져오기\n",
    "#    - 파일명: \"en-1.txt\" 형태\n",
    "# ============================================================\n",
    "TRAIN_DIR = \"./Data/train\"\n",
    "TEST_DIR  = \"./Data/test\"\n",
    "\n",
    "train_files = sorted([fn for fn in os.listdir(TRAIN_DIR) if fn.endswith(\".txt\")])\n",
    "test_files  = sorted([fn for fn in os.listdir(TEST_DIR)  if fn.endswith(\".txt\")])\n",
    "\n",
    "print(\"train_files:\", train_files[:5], \"...\")\n",
    "print(\"test_files :\", test_files[:5], \"...\")\n",
    "\n",
    "\n",
    "alphabet = [chr(ord('a') + i) for i in range(26)]  # a~z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "be69fd6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ============================================================\n",
    "# 2) 파일 읽어서 -> 26차원 알파벳 빈도 비율\n",
    "#    - 소문자로 치환\n",
    "#    - 악센트 -> 기본 알파벳\n",
    "#    - a~z만 남기기\n",
    "# ============================================================\n",
    "def normalize_char_simple(c):\n",
    "    return accent_map.get(c, c)\n",
    "\n",
    "def file_to_freq_vec(folder, filename):\n",
    "    with open(os.path.join(folder, filename), \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "        s = f.read()\n",
    "\n",
    "    # 알파벳만 + 소문자 + 악센트 매핑\n",
    "    chars = [normalize_char_simple(ch.lower()) for ch in s if ch.isalpha()]\n",
    "    \n",
    "    # 알파벳 외의 다른 문자들 지우기 ex) 한자\n",
    "    chars = [c for c in chars if 'a' <= c <= 'z']\n",
    "\n",
    "\n",
    "    # if len(chars) == 0:\n",
    "    #     return [0.0] * 26\n",
    "\n",
    "    cnt = Counter(chars)\n",
    "    total = sum(cnt.values())\n",
    "    vec = [cnt.get(a, 0) / total for a in alphabet]  # 비율(정규화)\n",
    "    return vec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ca713d78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels: ['en', 'fr', 'id', 'tl']\n",
      "label_to_idx: {'en': 0, 'fr': 1, 'id': 2, 'tl': 3}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ============================================================\n",
    "# 3) 학습용 X,y 만들기\n",
    "#    - 라벨: 파일명 앞 2글자\n",
    "# ============================================================\n",
    "X_train, y_train_str = [], []\n",
    "for fn in train_files:\n",
    "    label = fn.split(\"-\")[0]  # 파일명에서 라벨 가져오기\n",
    "    vec = file_to_freq_vec(TRAIN_DIR, fn)\n",
    "    X_train.append(vec)\n",
    "    y_train_str.append(label)\n",
    "\n",
    "# 라벨 인코딩: 문자열 라벨 -> 정수 라벨\n",
    "labels = sorted(set(y_train_str))\n",
    "label_to_idx = {lab:i for i, lab in enumerate(labels)}\n",
    "idx_to_label = {i:lab for lab, i in label_to_idx.items()}\n",
    "\n",
    "y_train = [label_to_idx[lab] for lab in y_train_str]\n",
    "\n",
    "print(\"labels:\", labels)\n",
    "print(\"label_to_idx:\", label_to_idx)\n",
    "\n",
    "# ============================================================\n",
    "# 4) 테스트용 X,y 만들기\n",
    "# ============================================================\n",
    "X_test, y_test_str = [], []\n",
    "for fn in test_files:\n",
    "    label = fn.split(\"-\")[0]\n",
    "    vec = file_to_freq_vec(TEST_DIR, fn)\n",
    "    X_test.append(vec)\n",
    "    y_test_str.append(label)\n",
    "\n",
    "y_test = [label_to_idx[lab] for lab in y_test_str]  # train에 없는 라벨이 있으면 에러 (보통 없음)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3e5605e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ============================================================\n",
    "# 5) Tensor로 변환 + DataLoader\n",
    "# ============================================================\n",
    "X_train_ts = torch.tensor(X_train, dtype=torch.float32)  \n",
    "y_train_ts = torch.tensor(y_train, dtype=torch.long)     \n",
    "X_test_ts  = torch.tensor(X_test,  dtype=torch.float32)\n",
    "y_test_ts  = torch.tensor(y_test,  dtype=torch.long)\n",
    "\n",
    "train_loader = DataLoader(TensorDataset(X_train_ts, y_train_ts), batch_size=8, shuffle=True)\n",
    "test_loader  = DataLoader(TensorDataset(X_test_ts,  y_test_ts),  batch_size=16, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d0135297",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ============================================================\n",
    "# 6) 딥러닝 모델(MLP) 정의\n",
    "#    - 출력은 logits (CrossEntropyLoss가 softmax 포함)\n",
    "# ============================================================\n",
    "class LangMLP(nn.Module):\n",
    "    def __init__(self, in_dim=26, num_classes=4):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(in_dim, 64)\n",
    "        self.fc2 = nn.Linear(64, 32)\n",
    "        self.out = nn.Linear(32, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return self.out(x)  # logits\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = LangMLP(in_dim=26, num_classes=len(labels)).to(device)\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "opt = torch.optim.Adam(model.parameters(), lr=1e-3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c19c4d61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[001] train loss=1.3899, acc=0.2500 | test loss=1.3873, acc=0.2500\n",
      "[010] train loss=1.3857, acc=0.2500 | test loss=1.3853, acc=0.2500\n",
      "[020] train loss=1.3800, acc=0.2917 | test loss=1.3825, acc=0.2500\n",
      "[030] train loss=1.3757, acc=0.2917 | test loss=1.3780, acc=0.2500\n",
      "[040] train loss=1.3644, acc=0.6250 | test loss=1.3684, acc=0.6250\n",
      "[050] train loss=1.3410, acc=0.7083 | test loss=1.3463, acc=0.7500\n",
      "[060] train loss=1.2774, acc=0.8750 | test loss=1.2977, acc=1.0000\n",
      "[070] train loss=1.1916, acc=0.7083 | test loss=1.2109, acc=0.8750\n",
      "[080] train loss=1.0612, acc=0.7500 | test loss=1.0907, acc=0.8750\n",
      "[090] train loss=0.9084, acc=0.5417 | test loss=0.9678, acc=0.6250\n",
      "[100] train loss=0.8081, acc=0.5000 | test loss=0.8680, acc=0.6250\n",
      "[110] train loss=0.7187, acc=0.7500 | test loss=0.8038, acc=0.7500\n",
      "[120] train loss=0.6882, acc=0.7500 | test loss=0.7476, acc=1.0000\n",
      "[130] train loss=0.6240, acc=0.8750 | test loss=0.7020, acc=1.0000\n",
      "[140] train loss=0.5487, acc=1.0000 | test loss=0.6616, acc=1.0000\n",
      "[150] train loss=0.5337, acc=1.0000 | test loss=0.6195, acc=1.0000\n",
      "[160] train loss=0.4789, acc=0.9167 | test loss=0.5685, acc=1.0000\n",
      "[170] train loss=0.4506, acc=0.9583 | test loss=0.5429, acc=1.0000\n",
      "[180] train loss=0.3891, acc=0.9583 | test loss=0.4994, acc=1.0000\n",
      "[190] train loss=0.3466, acc=1.0000 | test loss=0.4535, acc=1.0000\n",
      "[200] train loss=0.3413, acc=0.9167 | test loss=0.4215, acc=1.0000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ============================================================\n",
    "# 7) 학습 루프 + 테스트 평가\n",
    "# ============================================================\n",
    "def accuracy(logits, y):\n",
    "    pred = logits.argmax(dim=1)\n",
    "    return (pred == y).float().mean().item()\n",
    "\n",
    "EPOCHS = 200\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    # ---- train\n",
    "    model.train()\n",
    "    tr_loss_sum, tr_acc_sum = 0.0, 0.0\n",
    "\n",
    "    for xb, yb in train_loader:\n",
    "        xb, yb = xb.to(device), yb.to(device)\n",
    "\n",
    "        logits = model(xb)\n",
    "        loss = loss_fn(logits, yb)\n",
    "\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "\n",
    "        tr_loss_sum += loss.item()\n",
    "        tr_acc_sum += accuracy(logits, yb)\n",
    "\n",
    "    tr_loss = tr_loss_sum / len(train_loader)\n",
    "    tr_acc  = tr_acc_sum  / len(train_loader)\n",
    "\n",
    "    # ---- test\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        te_loss_sum, te_acc_sum = 0.0, 0.0\n",
    "        for xb, yb in test_loader:\n",
    "            xb, yb = xb.to(device), yb.to(device)\n",
    "            logits = model(xb)\n",
    "            loss = loss_fn(logits, yb)\n",
    "            te_loss_sum += loss.item()\n",
    "            te_acc_sum += accuracy(logits, yb)\n",
    "\n",
    "        te_loss = te_loss_sum / len(test_loader)\n",
    "        te_acc  = te_acc_sum  / len(test_loader)\n",
    "\n",
    "    if epoch % 10 == 0 or epoch == 1:\n",
    "        print(f\"[{epoch:03}] train loss={tr_loss:.4f}, acc={tr_acc:.4f} | test loss={te_loss:.4f}, acc={te_acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f10897dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "en-1.txt -> pred: en\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './Data/test2\\\\en-2.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 7\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m fn \u001b[38;5;129;01min\u001b[39;00m test_files[:\u001b[38;5;241m5\u001b[39m]:\n\u001b[1;32m----> 7\u001b[0m         vec \u001b[38;5;241m=\u001b[39m \u001b[43mfile_to_freq_vec\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m./Data/test2\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m         x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor([vec], dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m      9\u001b[0m         pred_idx \u001b[38;5;241m=\u001b[39m model(x)\u001b[38;5;241m.\u001b[39margmax(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mitem()\n",
      "Cell \u001b[1;32mIn[4], line 11\u001b[0m, in \u001b[0;36mfile_to_freq_vec\u001b[1;34m(folder, filename)\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mfile_to_freq_vec\u001b[39m(folder, filename):\n\u001b[1;32m---> 11\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfolder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mignore\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m     12\u001b[0m         s \u001b[38;5;241m=\u001b[39m f\u001b[38;5;241m.\u001b[39mread()\n\u001b[0;32m     14\u001b[0m     \u001b[38;5;66;03m# 알파벳만 + 소문자 + 악센트 매핑\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\kdt008\\anaconda3\\envs\\torch251\\lib\\site-packages\\IPython\\core\\interactiveshell.py:324\u001b[0m, in \u001b[0;36m_modified_open\u001b[1;34m(file, *args, **kwargs)\u001b[0m\n\u001b[0;32m    317\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[0;32m    318\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    319\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    320\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    321\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    322\u001b[0m     )\n\u001b[1;32m--> 324\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m io_open(file, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './Data/test2\\\\en-2.txt'"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# 8) 예측 확인\n",
    "# ============================================================\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for fn in test_files[:5]:\n",
    "        vec = file_to_freq_vec('./Data/test2', fn)\n",
    "        x = torch.tensor([vec], dtype=torch.float32).to(device)\n",
    "        pred_idx = model(x).argmax(dim=1).item()\n",
    "        print(f\"{fn} -> pred: {idx_to_label[pred_idx]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch251",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
